{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-kMq3lWqeWZ"
      },
      "outputs": [],
      "source": [
        "# Databricks notebook source\n",
        "!pip install -q -U tqdm\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "!pip install -q -U datasets\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "!pip install --upgrade -q -U ragas\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "dbutils.library.restartPython()\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "from transformers import pipeline\n",
        "from deepeval.models.base_model import DeepEvalBaseLLM\n",
        "from deepeval.test_case import LLMTestCaseParams, LLMTestCase\n",
        "from deepeval import assert_test\n",
        "from deepeval.metrics import (\n",
        "    GEval, AnswerRelevancyMetric,\n",
        "    ContextualPrecisionMetric,\n",
        "    ContextualRecallMetric,\n",
        "    ContextualRelevancyMetric\n",
        ")\n",
        "import openai\n",
        "from openai import OpenAI, AzureOpenAI\n",
        "import os\n",
        "from langchain_openai import AzureChatOpenAI, ChatOpenAI\n",
        "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "#from dotenv import load_dotenv, find_dotenv\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate\n",
        ")\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.evaluation import load_evaluator, EvaluatorType\n",
        "from dotenv import load_dotenv\n",
        "from langchain_community.document_loaders import WebBaseLoader, TextLoader\n",
        "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
        "from langchain_community.embeddings.sentence_transformer import (\n",
        "    SentenceTransformerEmbeddings,\n",
        ")\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
        "import pandas as pd\n",
        "import giskard\n",
        "import os\n",
        "from giskard.rag import KnowledgeBase, generate_testset, evaluate\n",
        "from giskard.llm.client.openai import OpenAIClient\n",
        "from giskard.llm import set_llm_model\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from operator import itemgetter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
        "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
        "from tqdm import tqdm\n",
        "from datasets import Dataset\n",
        "from ragas.metrics import (\n",
        "    answer_relevancy,\n",
        "    faithfulness,\n",
        "    context_recall,\n",
        "    context_precision,\n",
        "    answer_relevancy,\n",
        "    answer_correctness,\n",
        "    answer_similarity\n",
        ")\n",
        "\n",
        "from ragas.metrics.critique import harmfulness\n",
        "from ragas import evaluate\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# Setting up the environment and teh model\n",
        "environment = dbutils.secrets.get(scope = \"kvsecretscope\", key = \"Environment\")\n",
        "openai.api_type = \"azure\"\n",
        "openai.api_version = \"2023-03-1\"\n",
        "client_id = dbutils.secrets.get(\"kvsecretscope\", \"OpenAI\")\n",
        "client_secret = dbutils.secrets.get(\"akvsecretscope\", \"APIKey\")\n",
        "openai.azure_endpoint = client_id\n",
        "openai.api_key = client_secret\n",
        "\n",
        "os.environ['AZURE_OPENAI_API_KEY'] = client_secret\n",
        "os.environ['AZURE_OPENAI_ENDPOINT'] = client_id\n",
        "os.environ['OPENAI_API_VERSION'] = \"2023-03-1\"\n",
        "os.environ['OPENAI_API_KEY'] = client_secret\n",
        "os.environ['DEEPEVAL_API_KEY'] = client_secret\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "custom_model = AzureChatOpenAI(\n",
        "    openai_api_version=openai.api_version,\n",
        "    azure_deployment=\"gpt-35-turbo\",\n",
        "    azure_endpoint=client_id,\n",
        "    openai_api_key=client_secret,\n",
        ")\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# Loading the data , split it into chunks, embedding and vectorStore\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=5)\n",
        "loader = WebBaseLoader(\"yahoo.com/en-nl\")\n",
        "documents = loader.load_and_split(text_splitter)\n",
        "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "db = Chroma.from_documents(documents, embedding_function)\n",
        "vectorstore = DocArrayInMemorySearch.from_documents(documents, embedding=embedding_function)\n",
        "retriever = db.as_retriever()\n",
        "retriever.get_relevant_documents(\"What is ML engineering?\")\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "def answer_fn(question, history=None):\n",
        "    return chain.invoke({\"question\": question})\n",
        "\n",
        "# Using Giskard for evaluation\n",
        "# Creating Knowledge base and test set for evaluation\n",
        "set_llm_model('gpt-35-turbo')\n",
        "df = pd.DataFrame([d.page_content for d in documents], columns=[\"text\"])\n",
        "knowledge_base = KnowledgeBase.from_pandas(df, columns=[\"text\"])\n",
        "testset = generate_testset(\n",
        "    knowledge_base,\n",
        "    num_questions=5,\n",
        "    agent_description=\"A chatbot answering questions about the Machienlearning and AI\",\n",
        ")\n",
        "test_set_df = testset.to_pandas()\n",
        "\n",
        "for index, row in enumerate(test_set_df.head(3).iterrows()):\n",
        "    print(f\"Question {index + 1}: {row[1]['question']}\")\n",
        "    print(f\"Reference answer: {row[1]['reference_answer']}\")\n",
        "    print(\"Reference context:\")\n",
        "    print(row[1]['reference_context'])\n",
        "    print(\"******************\", end=\"\\n\\n\")\n",
        "testset.save(\"test-set.jsonl\")\n",
        "\n",
        "template = \"\"\"\n",
        "Answer the question based on the context below. If you can't\n",
        "answer the question, reply \"Ask my Boss\".\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "# print(prompt.format(context=\"Here is some context\", question=\"Here is a question\"))\n",
        "\n",
        "retrieval_augmented_qa_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": prompt | custom_model, \"context\": itemgetter(\"context\")}\n",
        ")\n",
        "chain = (\n",
        "    {\n",
        "        \"context\": itemgetter(\"question\") | retriever,\n",
        "        \"question\": itemgetter(\"question\"),\n",
        "    }\n",
        "    | prompt\n",
        "    | custom_model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "chain.invoke({\"question\": \"What is RAG LLM?\"})\n",
        "\n",
        "# comparing the answers from the chain with the reference answers in the test set.\n",
        "report = giskard.rag.evaluate(answer_fn, testset=testset, knowledge_base=knowledge_base)\n",
        "report.correctness_by_question_type()\n",
        "# display(report)\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "question_schema = ResponseSchema(\n",
        "  name = \"question\",\n",
        "  description = \"A question about the context\"\n",
        ")\n",
        "question_response_schemas = [question_schema]\n",
        "question_output_parser = StructuredOutputParser.from_response_schemas(question_response_schemas)\n",
        "format_instructions = question_output_parser.get_format_instructions()\n",
        "question_generation_llm = AzureChatOpenAI(model=\"gpt-35-turbo\", openai_api_key=client_secret)\n",
        "\n",
        "bare_prompt_template = \"{content}\"\n",
        "bare_template = ChatPromptTemplate.from_template(template=bare_prompt_template)\n",
        "\n",
        "qa_template = \"\"\"\\\n",
        "You are a University Professor creating a test for advanced students. For each context,\n",
        "create a question that is specific to the context. Avoid creating generic or general questions.\n",
        "\n",
        "question: a question about the context.\n",
        "\n",
        "Format the output as JSON with the following keys:\n",
        "question\n",
        "\n",
        "context: {context}\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(template=qa_template)\n",
        "\n",
        "messages = prompt_template.format_messages(\n",
        "    context=documents[0],\n",
        "    format_instructions=format_instructions\n",
        ")\n",
        "\n",
        "question_generation_chain = bare_template | question_generation_llm\n",
        "\n",
        "response = question_generation_chain.invoke({\"content\" : messages})\n",
        "output_dict = question_output_parser.parse(response.content)\n",
        "\n",
        "qac_triples = []\n",
        "\n",
        "for text in tqdm(documents[:10]):\n",
        "  messages = prompt_template.format_messages(\n",
        "      context=text,\n",
        "      format_instructions=format_instructions\n",
        "  )\n",
        "  response = question_generation_chain.invoke({\"content\" : messages})\n",
        "  try:\n",
        "    output_dict = question_output_parser.parse(response.content)\n",
        "  except Exception as e:\n",
        "    continue\n",
        "  output_dict[\"context\"] = text\n",
        "  qac_triples.append(output_dict)\n",
        "\n",
        "answer_generation_llm = AzureChatOpenAI(model=\"gpt-35-turbo\", openai_api_key=client_secret)\n",
        "\n",
        "answer_schema = ResponseSchema(\n",
        "    name=\"answer\",\n",
        "    description=\"an answer to the question\"\n",
        ")\n",
        "\n",
        "answer_response_schemas = [\n",
        "    answer_schema,\n",
        "]\n",
        "\n",
        "answer_output_parser = StructuredOutputParser.from_response_schemas(answer_response_schemas)\n",
        "format_instructions = answer_output_parser.get_format_instructions()\n",
        "\n",
        "answer_qa_template = \"\"\"\\\n",
        "You are a University Professor creating a test for advanced students. For each question and context, create an answer.\n",
        "\n",
        "answer: a answer about the context.\n",
        "\n",
        "Format the output as JSON with the following keys:\n",
        "answer\n",
        "\n",
        "question: {question}\n",
        "context: {context}\n",
        "\"\"\"\n",
        "\n",
        "answer_prompt_template = ChatPromptTemplate.from_template(template=answer_qa_template)\n",
        "\n",
        "messages = answer_prompt_template.format_messages(\n",
        "    context=qac_triples[0][\"context\"],\n",
        "    question=qac_triples[0][\"question\"],\n",
        "    format_instructions=format_instructions\n",
        ")\n",
        "\n",
        "answer_generation_chain = bare_template | answer_generation_llm\n",
        "\n",
        "answer_response = answer_generation_chain.invoke({\"content\" : messages})\n",
        "output_dict = answer_output_parser.parse(answer_response.content)\n",
        "\n",
        "for triple in tqdm(qac_triples):\n",
        "  messages = answer_prompt_template.format_messages(\n",
        "      context=triple[\"context\"],\n",
        "      question=triple[\"question\"],\n",
        "      format_instructions=format_instructions\n",
        "  )\n",
        "  response = answer_generation_chain.invoke({\"content\" : messages})\n",
        "  try:\n",
        "    output_dict = answer_output_parser.parse(response.content)\n",
        "  except Exception as e:\n",
        "    continue\n",
        "  triple[\"answer\"] = output_dict[\"answer\"]\n",
        "\n",
        "ground_truth_qac_set = pd.DataFrame(qac_triples)\n",
        "ground_truth_qac_set[\"context\"] = ground_truth_qac_set[\"context\"].map(lambda x: str(x.page_content))\n",
        "ground_truth_qac_set = ground_truth_qac_set.rename(columns={\"answer\" : \"ground_truth\"})\n",
        "\n",
        "\n",
        "eval_dataset = Dataset.from_pandas(ground_truth_qac_set)\n",
        "eval_dataset.to_csv(\"./groundtruth_eval_dataset.csv\")\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "def create_ragas_dataset(rag_pipeline, eval_dataset):\n",
        "  rag_dataset = []\n",
        "  for row in tqdm(eval_dataset):\n",
        "    answer = rag_pipeline.invoke({\"question\" : row[\"question\"]})\n",
        "    rag_dataset.append(\n",
        "        {\"question\" : row[\"question\"],\n",
        "         \"answer\" : answer[\"response\"].content,\n",
        "         \"contexts\" : [context.page_content for context in answer[\"context\"]],\n",
        "         \"ground_truths\" : [row[\"ground_truth\"]]\n",
        "         }\n",
        "    )\n",
        "  rag_df = pd.DataFrame(rag_dataset)\n",
        "  rag_eval_dataset = Dataset.from_pandas(rag_df)\n",
        "  return rag_eval_dataset\n",
        "\n",
        "from ragas import evaluate\n",
        "\n",
        "def evaluate_ragas_dataset(ragas_dataset):\n",
        "  result = evaluate(\n",
        "    ragas_dataset,\n",
        "    metrics=[\n",
        "        context_precision,\n",
        "        faithfulness,\n",
        "        answer_relevancy,\n",
        "        context_recall,\n",
        "        answer_relevancy,\n",
        "        answer_correctness,\n",
        "        answer_similarity\n",
        "    ],\n",
        "  )\n",
        "  return result\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "basic_qa_ragas_dataset = create_ragas_dataset(retrieval_augmented_qa_chain, eval_dataset)\n",
        "basic_qa_ragas_dataset.to_csv(\"./basic_qa_ragas_dataset.csv\")\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# basic_qa_result = evaluate_ragas_dataset(basic_qa_ragas_dataset)\n",
        "metrics=[\n",
        "        context_precision,\n",
        "        faithfulness,\n",
        "        answer_relevancy,\n",
        "        context_recall,\n",
        "        answer_relevancy,\n",
        "        answer_correctness,\n",
        "        answer_similarity\n",
        "    ]\n",
        "\n",
        "result = evaluate(\n",
        "    basic_qa_ragas_dataset, metrics=metrics, llm=custom_model, embeddings=embedding_function\n",
        ")\n",
        "\n",
        "result\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "\n",
        "def create_qa_chain(retriever):\n",
        "  primary_qa_llm = AzureChatOpenAI(model=\"gpt-35-turbo\", openai_api_key=client_secret, temperature=0)\n",
        "  created_qa_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | retriever,\n",
        "     \"question\": itemgetter(\"question\")\n",
        "    }\n",
        "    | RunnablePassthrough.assign(\n",
        "        context=itemgetter(\"context\")\n",
        "      )\n",
        "    | {\n",
        "         \"response\": prompt | primary_qa_llm,\n",
        "         \"context\": itemgetter(\"context\"),\n",
        "      }\n",
        "  )\n",
        "\n",
        "  return created_qa_chain\n",
        "\n",
        "\n",
        "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=1500)\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=200)\n",
        "\n",
        "vectorstore = Chroma(collection_name=\"split_parents\", embedding_function=embedding_function)\n",
        "\n",
        "store = InMemoryStore()\n",
        "\n",
        "parent_document_retriever = ParentDocumentRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        "    parent_splitter=parent_splitter,\n",
        ")\n",
        "parent_document_retriever.add_documents(documents)\n",
        "parent_document_retriever_qa_chain = create_qa_chain(parent_document_retriever)\n",
        "parent_document_retriever_qa_chain.invoke({\"question\" : \"What is RAG?\"})[\"response\"].content\n",
        "pdr_qa_ragas_dataset = create_ragas_dataset(parent_document_retriever_qa_chain, eval_dataset)\n",
        "pdr_qa_ragas_dataset.to_csv(\"./pdr_qa_ragas_dataset.csv\")\n",
        "metrics=[\n",
        "        context_precision,\n",
        "        faithfulness,\n",
        "        answer_relevancy,\n",
        "        context_recall,\n",
        "        answer_relevancy,\n",
        "        answer_correctness,\n",
        "        answer_similarity\n",
        "    ]\n",
        "\n",
        "pdr_qa_result = evaluate(\n",
        "    pdr_qa_ragas_dataset, metrics=metrics, llm=custom_model, embeddings=embedding_function\n",
        ")\n",
        "\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "pdr_qa_result\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# Using DeepEval\n",
        "class AzureOpenAI(DeepEvalBaseLLM):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model\n",
        "    ):\n",
        "        self.model = model\n",
        "\n",
        "    def load_model(self):\n",
        "        return self.model\n",
        "\n",
        "    def generate(self, prompt: str) -> str:\n",
        "        chat_model = self.load_model()\n",
        "        return chat_model.invoke(prompt).content\n",
        "\n",
        "    async def a_generate(self, prompt: str) -> str:\n",
        "        chat_model = self.load_model()\n",
        "        res = await chat_model.ainvoke(prompt)\n",
        "        return res.content\n",
        "\n",
        "    def get_model_name(self):\n",
        "        return \"Custom Azure OpenAI Model\"\n",
        "\n",
        "azure_openai = AzureOpenAI(model=custom_model)\n",
        "\n",
        "actual_output = \"We offer a 30-day full refund at no extra cost.\"\n",
        "answer_rel_metric = AnswerRelevancyMetric(model=azure_openai, threshold=0.7)\n",
        "# evaluates the reranker in retriever\n",
        "contextual_precision = ContextualPrecisionMetric(model=azure_openai)\n",
        "# evaluates the embedding model in retriever\n",
        "contextual_recall = ContextualRecallMetric(model=azure_openai)\n",
        "# evaluates the text chunk size and top-K of retriever\n",
        "contextual_relevancy = ContextualRelevancyMetric(model=azure_openai)\n",
        "correctness_metric = GEval(\n",
        "  name=\"Correctness\",\n",
        "  model = azure_openai,\n",
        "  criteria=\"Determine whether the actual output is factually correct based on the expected output.\",\n",
        "  evaluation_steps=[\n",
        "    \"Check whether the facts in 'actual output' contradicts any facts in 'expected output'\",\n",
        "    \"You should also heavily penalize omission of detail\",\n",
        "    \"Vague language, or contradicting OPINIONS, are OK\"\n",
        "    ],\n",
        "  evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT]\n",
        "  )\n",
        "\n",
        "test_case = LLMTestCase(\n",
        "    input=\"What is Desicion tree?\",# \"The dog chased the cat up the tree, who ran up the tree?\"\n",
        "    actual_output=actual_output # , expected_output=\"The cat.\"\n",
        ")\n",
        "\n",
        "correctness_metric.measure(test_case)\n",
        "# answer_rel_metric.measure(test_case)\n",
        "# print(correctness_metric.score)\n",
        "# print(correctness_metric.reason)\n",
        "# print(metric.score)\n",
        "# print(metric.reason)\n",
        "# correctness_metric.is_successful()\n",
        "\n",
        "from deepeval import evaluate\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=custom_model,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        ")\n",
        "queries = [\"Tell me about the Eiffel Tower.\", \"What is RAG?\"]\n",
        "\n",
        "evaluation_results = []\n",
        "for query in queries:\n",
        "    response = qa.run(query)\n",
        "    test_case = LLMTestCase(\n",
        "    input=query,\n",
        "    actual_output= response,\n",
        "    retrieval_context=[\n",
        "        \"\"\"If you are an AI engineer you would know that this is teh best solution ever\"\"\"\n",
        "    ]\n",
        "    )\n",
        "    # evaluation = g_eval(response, query)\n",
        "    evaluation = evaluate([test_case],[correctness_metric])\n",
        "    evaluation_results.append((query, response, evaluation))\n",
        "\n",
        "for query, response, result in evaluation_results:\n",
        "    print(f\"New Query: {query}\")\n",
        "    print(f\"New Response: {response}\")\n",
        "    print(f\"New Evaluation: {result}\\n\")\n",
        "    # print(f\"Correctness score: {score:.2f}\")\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "template = \"You are a teacher. Give a brief answer to anything asked\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "human_template = \"{text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
        "chain = LLMChain(llm = custom_model, prompt=chat_prompt)\n",
        "prompts = {'conciseness':['What is machien learning?','What is the capital of the Netherlands?'],\n",
        "           'relevance':['what are the ingredients of  pizza margarita'],\n",
        "           'coherence':['What is machien learning?','What is the capital of the Netherlands?']}\n",
        "for criteria in prompts:\n",
        "  evaluator = load_evaluator(EvaluatorType.CRITERIA, llm=custom_model, criteria=criteria)\n",
        "  print(\"\\n**{}**\".format(criteria.upper()))\n",
        "  for prompt in prompts[criteria]:\n",
        "    prediction = chain.run(prompt)\n",
        "    eval_result = evaluator.evaluate_strings(\n",
        "      prediction=prediction,\n",
        "      input=prompt\n",
        "    )\n",
        "    print(\"\\nPROMPT :\", prompt)\n",
        "    print(\"\\nRESULT :\", '\\n'.join(prediction.replace('\\n','').split('.')[:-1]))\n",
        "    print(\"\\nVALUE :\", eval_result['value'])\n",
        "    print(\"\\nSCORE :\", eval_result['score'])\n",
        "    print(\"\\nREASON :\",'\\n'.join(eval_result['reasoning'].replace('\\n','').split('.')[:-1]))\n",
        "\n",
        "prompts2 = {'correctness':{'prompt':\"How many players are required for chess?\",'answer':'2'},\n",
        "           'relevance':{'prompt':\"What is a data science?\",'answer':'Data science is an interdiciplinary field that combines math, statistics,\\ programming, advanced analytics and artificial intelligence(AI)'}}\n",
        "for criteria in prompts2:\n",
        "  evaluator = load_evaluator(\"labeled_criteria\", llm=custom_model, criteria=criteria)\n",
        "  print(\"\\n**{}**\".format(criteria.upper()))\n",
        "  prediction = chain.run(prompts2[criteria]['prompt'])\n",
        "  eval_result = evaluator.evaluate_strings(\n",
        "    prediction=prediction,\n",
        "    input=prompts2[criteria]['prompt'],\n",
        "    reference= prompts2[criteria]['answer']\n",
        "  )\n",
        "  print(\"\\nPROMPT :\", prompts2[criteria]['prompt'])\n",
        "  print(\"\\nRESULT :\", '\\n'.join(prediction.replace('\\n','').split('.')[:-1]))\n",
        "  print(\"\\nVALUE :\", eval_result['value'])\n",
        "  print(\"\\nSCORE :\", eval_result['score'])\n",
        "  print(\"\\nREASON :\",'\\n'.join(eval_result['reasoning'].replace('\\n','').split('.')[:-1]))\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# MAGIC %environment\n",
        "# MAGIC \"client\": \"1\"\n",
        "# MAGIC \"base_environment\": \"\"\n"
      ]
    }
  ]
}