{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pkkm1Nsaprit"
      },
      "outputs": [],
      "source": [
        "# Databricks notebook source\n",
        "from transformers import pipeline\n",
        "from deepeval.models.base_model import DeepEvalBaseLLM\n",
        "from deepeval.test_case import LLMTestCaseParams, LLMTestCase\n",
        "from deepeval import assert_test\n",
        "from deepeval.metrics import (\n",
        "    GEval, AnswerRelevancyMetric,\n",
        "    ContextualPrecisionMetric,\n",
        "    ContextualRecallMetric,\n",
        "    ContextualRelevancyMetric\n",
        ")\n",
        "import openai\n",
        "from openai import OpenAI, AzureOpenAI\n",
        "import os\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "environment = dbutils.secrets.get(scope = \"kvsecretscope\", key = \"Environment\")\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "openai.api_type = \"azure\"\n",
        "openai.api_version = \"2023-03-1\"\n",
        "client_id = dbutils.secrets.get(\"akvsecretscope01\", \"OpenAI\")\n",
        "client_secret = dbutils.secrets.get(\"akvsecretscope01\", \"APIKey\")\n",
        "openai.azure_endpoint = client_id\n",
        "openai.api_key = client_secret\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "completion = openai.chat.completions.create(\n",
        "            model=\"gpt-35-turbo\",\n",
        "            messages = [{\n",
        "            \"role\": \"user\",\n",
        "      \"content\": \"What are the differences between Azure Machine Learning and Azure AI services?\"}\n",
        "        ],\n",
        "            max_tokens=500,\n",
        "            n=1,\n",
        "            stop=None,\n",
        "            temperature=0\n",
        "        )\n",
        "print(completion.choices[0].message.content)\n",
        "# print(completion.to_json())\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "class AzureOpenAI(DeepEvalBaseLLM):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model\n",
        "    ):\n",
        "        self.model = model\n",
        "\n",
        "    def load_model(self):\n",
        "        return self.model\n",
        "\n",
        "    def generate(self, prompt: str) -> str:\n",
        "        chat_model = self.load_model()\n",
        "        return chat_model.invoke(prompt).content\n",
        "\n",
        "    async def a_generate(self, prompt: str) -> str:\n",
        "        chat_model = self.load_model()\n",
        "        res = await chat_model.ainvoke(prompt)\n",
        "        return res.content\n",
        "\n",
        "    def get_model_name(self):\n",
        "        return \"Custom Azure OpenAI Model\"\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# Replace these with real values\n",
        "custom_model = AzureChatOpenAI(\n",
        "    openai_api_version=openai.api_version,\n",
        "    azure_deployment=\"gpt-35-turbo\",\n",
        "    azure_endpoint=client_id,\n",
        "    openai_api_key=client_secret,\n",
        ")\n",
        "azure_openai = AzureOpenAI(model=custom_model)\n",
        "print(azure_openai.generate(\"Write me a joke\"))\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# Sample Chatbot Class using Hugging Face model\n",
        "class HuggingFaceChatBot:\n",
        "    def __init__(self, qa_pipeline):\n",
        "        self.qa_pipeline = qa_pipeline\n",
        "        self.context = (\"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France.\\\n",
        "        It is named after the engineer Gustave Eiffel, whose company designed and built the tower. \\\n",
        "        Constructed from 1887 to 1889 as the entrance arch for the 1889 World's Fair, it was initially criticized \\\n",
        "        by some of France's leading artists and intellectuals for its design, but it has become a global cultural icon\\\n",
        "        of France and one of the most recognizable structures in the world. \\\n",
        "        The Eiffel Tower is the most-visited paid monument in the world; 6.91 million people ascended it in 2015.\\\n",
        "         The tower is 330 meters (1,083 ft) tall, about the same height as an 81-story building, and the tallest structure \\\n",
        "         in Paris. Its base is square, measuring 125 meters (410 ft) on each side.\")\n",
        "\n",
        "\n",
        "    def get_response(self, query):\n",
        "        response = self.qa_pipeline(question=query, context=self.context)\n",
        "        return response['answer']\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "\n",
        "answer_relevancy_metric = AnswerRelevancyMetric(model=azure_openai, threshold=0.5)\n",
        "test_case = LLMTestCase(\n",
        "    input=\"What if these shoes don't fit?\",\n",
        "    # Replace this with the actual output of your LLM application\n",
        "    actual_output=\"We  dont care\"\n",
        ")\n",
        "# assert_test(test_case, [answer_relevancy_metric])\n",
        "answer_relevancy_metric.measure(test_case)\n",
        "print(answer_relevancy_metric.score)\n",
        "print(answer_relevancy_metric.reason)\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "metric = AnswerRelevancyMetric(model=azure_openai)\n",
        "test_case = LLMTestCase(\n",
        "  input=\"The dog chased the cat up the tree, who ran up the tree?\",\n",
        "  actual_output=\"It depends, some might consider the cat, while others might argue the dog.\",\n",
        "  expected_output=\"The cat.\"\n",
        "  )\n",
        "metric.measure(test_case)\n",
        "print(metric.score)\n",
        "print(metric.reason)\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# evaluates the reranker in retriever\n",
        "contextual_precision = ContextualPrecisionMetric(model=azure_openai)\n",
        "# evaluates the embedding model in retriever\n",
        "contextual_recall = ContextualRecallMetric(model=azure_openai)\n",
        "# evaluates the text chunk size and top-K of retriever\n",
        "contextual_relevancy = ContextualRelevancyMetric(model=azure_openai)\n",
        "\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "test_case = LLMTestCase(\n",
        "    input=\"I'm on an F-1 visa, gow long can I stay in the US after graduation?\",\n",
        "      # the final generation of your RAG pipeline\n",
        "    actual_output=\"You can stay up to 30 days after completing your degree.\",\n",
        "    expected_output=\"You can stay up to 60 days after completing your degree.\",\n",
        "    # the retrieved text chunks during the retrieval step\n",
        "    retrieval_context=[\n",
        "        \"\"\"If you are in the U.S. on an F-1 visa, you are allowed to stay for 60 days after completing\n",
        "        your degree, unless you have applied for and been approved to participate in OPT.\"\"\"\n",
        "    ]\n",
        ")\n",
        "evaluate(\n",
        "    test_cases=[test_case],\n",
        "    metrics=[contextual_precision, contextual_recall, contextual_relevancy]\n",
        ")\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "correctness_metric = GEval(\n",
        "  name=\"Correctness\",\n",
        "  model = azure_openai,\n",
        "  criteria=\"Determine whether the actual output is factually correct based on the expected output.\",\n",
        "  evaluation_steps=[\n",
        "    \"Check whether the facts in 'actual output' contradicts any facts in 'expected output'\",\n",
        "    \"You should also heavily penalize omission of detail\",\n",
        "    \"Vague language, or contradicting OPINIONS, are OK\"\n",
        "    ],\n",
        "  evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT]\n",
        "  )\n",
        "\n",
        "test_case = LLMTestCase(\n",
        "  input=\"The dog chased the cat up the tree, who ran up the tree?\",\n",
        "  actual_output=\"It depends, some might consider the cat, while others might argue the dog.\",\n",
        "  expected_output=\"The cat.\"\n",
        "  )\n",
        "\n",
        "# correctness_metric.measure(test_case)\n",
        "# print(correctness_metric.score)\n",
        "# print(correctness_metric.reason)\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# Initialize chatbot\n",
        "chatbot = HuggingFaceChatBot(qa_pipeline)\n",
        "\n",
        "# Define queries for evaluation\n",
        "queries = [\"Tell me about the Eiffel Tower.\", \"What's the weather like today?\"]\n",
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "# Run the evaluation\n",
        "evaluation_results = []\n",
        "for query in queries:\n",
        "    response = chatbot.get_response(query)\n",
        "    # evaluation = g_eval(response, query)\n",
        "    evaluation = correctness_metric.evaluate(response)\n",
        "    evaluation_results.append((query, response, evaluation))\n",
        "# Print evaluation results\n",
        "for query, response, result in evaluation_results:\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Response: {response}\")\n",
        "    print(f\"Evaluation: {result}\\n\")\n",
        "    # print(f\"Correctness score: {score:.2f}\")\n"
      ]
    }
  ]
}