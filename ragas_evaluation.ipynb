{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_x6kUAa_dBct"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "from operator import itemgetter\n",
        "from tqdm import tqdm\n",
        "from datasets import Dataset\n",
        "import datetime\n",
        "import uuid\n",
        "from scipy.stats import hmean\n",
        "from typing import List, Dict, Any, Callable\n",
        "from azure.search.documents import SearchClient\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
        "from langchain_community.vectorstores.azuresearch import AzureSearch\n",
        "from langchain_community.retrievers import AzureAISearchRetriever\n",
        "from langchain.prompts.chat import ChatPromptTemplate\n",
        "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
        "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    answer_relevancy,\n",
        "    faithfulness,\n",
        "    context_recall,\n",
        "    context_precision,\n",
        "    answer_relevancy,\n",
        "    answer_correctness,\n",
        "    answer_similarity,\n",
        "    context_utilization,\n",
        "    context_entity_recall\n",
        ")\n",
        "from ragas.metrics.critique import harmfulness, coherence, conciseness, correctness\n",
        "from configs.widgets import *\n",
        "\n",
        "\n",
        "def create_ragas_dataset(rag_pipeline:Callable, eval_dataset:List[Dict[str, Any]])-> Dataset:\n",
        "  \"\"\"\n",
        "    Creates a RAGAS dataset from a given evaluation dataset using a specified RAG pipeline.\n",
        "\n",
        "    Args:\n",
        "        rag_pipeline (RagRetriever): The RAG pipeline to use for generating answers.\n",
        "        eval_dataset (List[Dict[str, Any]]): The evaluation dataset to process.\n",
        "        Each dictionary should contain a \"question\" and a \"ground_truth\".\n",
        "\n",
        "    Returns:\n",
        "        Dataset: A Dataset containing the questions, generated answers, contexts, and ground truths.\n",
        "\n",
        "    \"\"\"\n",
        "  rag_dataset = []\n",
        "  for row in tqdm(eval_dataset):\n",
        "    answer = rag_pipeline.invoke({\"question\" : row[\"question\"]})\n",
        "    rag_dataset.append(\n",
        "        {\"question\" : row[\"question\"],\n",
        "         \"answer\" : answer[\"response\"].content,\n",
        "         \"contexts\" : [context.page_content for context in answer[\"context\"]],\n",
        "         \"ground_truths\" : [row[\"ground_truth\"]]\n",
        "         }\n",
        "    )\n",
        "  rag_df = pd.DataFrame(rag_dataset)\n",
        "  rag_eval_dataset = Dataset.from_pandas(rag_df)\n",
        "  return rag_eval_dataset\n",
        "\n",
        "\n",
        "def calculate_ragas_score(answer_relevancy:float,\n",
        "                          faithfulness:float,\n",
        "                          contextual_precision:float,\n",
        "                          contextual_recall:float,\n",
        "                          answer_correctness:float,\n",
        "                          answer_similarity:float)->float:\n",
        "  \"\"\"\n",
        "    Calculate the RAGAS score as the harmonic mean of six metrics.\n",
        "\n",
        "    Parameters:\n",
        "    answer_relevancy (float): The answer relevancy metric.\n",
        "    faithfulness (float): The faithfulness metric.\n",
        "    contextual_precision (float): The contextual precision metric.\n",
        "    contextual_recall (float): The contextual recall metric.\n",
        "    answer_correctness (float): The answer correctness metric.\n",
        "    answer_similarity (float): The answer similarity metric.\n",
        "\n",
        "    Returns:\n",
        "    float: The RAGAS score.\n",
        "\n",
        "    Raises:\n",
        "    AssertionError: If any of the input values are less than or equal to 0.\n",
        "    \"\"\"\n",
        "\n",
        "  # Ensure all values are greater than 0, as the harmonic mean is undefined for values <= 0\n",
        "  assert all(i > 0 for i in [answer_relevancy, faithfulness, contextual_precision, contextual_recall]), \"All input values mustbe greater than 0\"\n",
        "\n",
        "  ragas_score = hmean([answer_relevancy, faithfulness,\n",
        "                       contextual_precision, contextual_recall,\n",
        "                       answer_correctness, answer_similarity])\n",
        "\n",
        "  return ragas_score\n",
        "\n",
        "\n",
        "  # Setting up the environment and the model\n",
        "environment = dbutils.secrets.get(scope = \"scope01\", key = \"Environment\")\n",
        "client_id = dbutils.secrets.get(\"scope01\", \"OpenAIEndpoint\")\n",
        "client_secret = dbutils.secrets.get(\"scope01\", \"OpenAIKey\")\n",
        "\n",
        "custom_model = AzureChatOpenAI(\n",
        "    openai_api_version=openai_api_version_widget.value,\n",
        "    azure_deployment=azure_deploy_widget.value,\n",
        "    azure_endpoint=client_id,\n",
        "    openai_api_key=client_secret,\n",
        ")\n",
        "embeddings: AzureOpenAIEmbeddings = AzureOpenAIEmbeddings(\n",
        "    azure_deployment=azure_ai_search_model_name_widget.value,\n",
        "    openai_api_version=azure_ai_search_api_version_widget.value,\n",
        "    azure_endpoint=azure_ai_search_endpoint_widget.value,\n",
        "    api_key=azure_ai_embedding_api_key_widget.value,\n",
        ")\n",
        "vector_store: AzureSearch = AzureSearch(\n",
        "    azure_search_endpoint=ai_search_vectorstore_address_widget.value,\n",
        "    azure_search_key=azure_ai_search_api_key_widget.value,\n",
        "    index_name=azure_ai_search_index_name_widget.value,\n",
        "    embedding_function=embeddings.embed_query,\n",
        "    # Configure max retries for the Azure client\n",
        "    additional_search_client_options={\"retry_total\": 4},\n",
        ")\n",
        "retriever = AzureAISearchRetriever(\n",
        "  service_name = azure_ai_search_service_name_widget.value,\n",
        "  api_key=azure_ai_search_api_key_widget.value,\n",
        "  content_key=\"chunk\",\n",
        "  top_k=1,\n",
        "  index_name=azure_ai_search_service_index_name_widget.value\n",
        ")\n",
        "\n",
        "search_client = SearchClient(\n",
        "    endpoint=ai_search_vectorstore_address_widget.value,\n",
        "    index_name=azure_ai_search_service_index_name_widget.value,\n",
        "    credential=AzureKeyCredential(azure_ai_search_api_key_widget.value)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = search_client.search(search_text=\"*\", include_total_count=True)  # Adjust the query as needed\n",
        "documents = []\n",
        "for result in results:\n",
        "    content = result.copy()\n",
        "    documents.append(content)\n",
        "\n",
        "# Question generation\n",
        "question_schema = ResponseSchema(\n",
        "  name = \"question\",\n",
        "  description = \"A question about the context\"\n",
        ")\n",
        "question_response_schemas = [question_schema]\n",
        "question_output_parser = StructuredOutputParser.from_response_schemas(question_response_schemas)\n",
        "format_instructions = question_output_parser.get_format_instructions()\n",
        "question_generation_llm = custom_model\n",
        "bare_prompt_template = \"{content}\"\n",
        "bare_template = ChatPromptTemplate.from_template(template=bare_prompt_template)\n",
        "qa_template = \"\"\"\\\n",
        "You are a University Professor creating a test for advanced students. For each context,\n",
        "create a question that is specific to the context. Avoid creating generic or general questions.\n",
        "question: a question about the context.\n",
        "Format the output as JSON with the following keys:\n",
        "question\n",
        "context: {context}\n",
        "\"\"\"\n",
        "prompt_template = ChatPromptTemplate.from_template(template=qa_template)\n",
        "messages = prompt_template.format_messages(\n",
        "    context=documents[0],\n",
        "    format_instructions=format_instructions\n",
        ")\n",
        "question_generation_chain = bare_template | question_generation_llm\n",
        "response = question_generation_chain.invoke({\"content\" : messages})\n",
        "output_dict = question_output_parser.parse(response.content)\n",
        "qac_triples = []\n",
        "for text in tqdm(documents[:10]):\n",
        "  messages = prompt_template.format_messages(\n",
        "      context=text,\n",
        "      format_instructions=format_instructions\n",
        "  )\n",
        "  response = question_generation_chain.invoke({\"content\" : messages})\n",
        "  try:\n",
        "    output_dict = question_output_parser.parse(response.content)\n",
        "  except Exception as e:\n",
        "    continue\n",
        "  output_dict[\"context\"] = text\n",
        "  qac_triples.append(output_dict)\n",
        "\n",
        "# Answer generation\n",
        "answer_generation_llm = custom_model\n",
        "answer_schema = ResponseSchema(\n",
        "    name=\"answer\",\n",
        "    description=\"an answer to the question\"\n",
        ")\n",
        "answer_response_schemas = [answer_schema]\n",
        "answer_output_parser = StructuredOutputParser.from_response_schemas(answer_response_schemas)\n",
        "format_instructions = answer_output_parser.get_format_instructions()\n",
        "answer_qa_template = \"\"\"\\\n",
        "You are a University Professor creating a test for advanced students. For each question and context, create an answer.\n",
        "answer: a answer about the context.\n",
        "Format the output as JSON with the following keys:\n",
        "answer\n",
        "question: {question}\n",
        "context: {context}\n",
        "\"\"\"\n",
        "answer_prompt_template = ChatPromptTemplate.from_template(template=answer_qa_template)\n",
        "messages = answer_prompt_template.format_messages(\n",
        "    context=qac_triples[0][\"context\"],\n",
        "    question=qac_triples[0][\"question\"],\n",
        "    format_instructions=format_instructions\n",
        ")\n",
        "answer_generation_chain = bare_template | answer_generation_llm\n",
        "answer_response = answer_generation_chain.invoke({\"content\" : messages})\n",
        "output_dict = answer_output_parser.parse(answer_response.content)\n",
        "for triple in tqdm(qac_triples):\n",
        "  messages = answer_prompt_template.format_messages(\n",
        "      context=triple[\"context\"],\n",
        "      question=triple[\"question\"],\n",
        "      format_instructions=format_instructions\n",
        "  )\n",
        "  response = answer_generation_chain.invoke({\"content\" : messages})\n",
        "  try:\n",
        "    output_dict = answer_output_parser.parse(response.content)\n",
        "  except Exception as e:\n",
        "    continue\n",
        "  triple[\"answer\"] = output_dict[\"answer\"]\n",
        "\n",
        "# Evaluation dataset generation\n",
        "ground_truth_qac_set = pd.DataFrame(qac_triples)\n",
        "ground_truth_qac_set[\"context\"] = ground_truth_qac_set[\"context\"].map(lambda x: str(x['chunk']))\n",
        "ground_truth_qac_set = ground_truth_qac_set.rename(columns={\"answer\" : \"ground_truth\"})\n",
        "eval_dataset = Dataset.from_pandas(ground_truth_qac_set)\n",
        "\n",
        "template = f\"\"\"\n",
        "{system_message_widget.value}\n",
        "Context: {{context}}\n",
        "Question: {{question}}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "retrieval_augmented_qa_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": prompt | custom_model, \"context\": itemgetter(\"context\")}\n",
        ")\n",
        "metrics=[\n",
        "      context_precision,# Retrieval, relevance and ranking of retrieved context chunks.\n",
        "      faithfulness, # = Generative,coherence, factual consistency of the generated answer against the given context\n",
        "      answer_relevancy,# Generative, relevance , the quality and conciseness of the retrieved context.\n",
        "      context_recall,# Retrieval accuracy, retrieval of relevant context chunks\n",
        "      answer_correctness,# Generative, groundness , accuracy of the generated answer when compared to the ground truth\n",
        "      answer_similarity,# Generative, semantic resemblance between the generated answer and the ground truth.\n",
        "      context_utilization, context_entity_recall,\n",
        "      harmfulness, coherence, conciseness, correctness\n",
        "      ]\n",
        "basic_qa_ragas_dataset = create_ragas_dataset(retrieval_augmented_qa_chain, eval_dataset)\n",
        "basic_qa_result = evaluate(basic_qa_ragas_dataset, metrics=metrics, llm=custom_model, embeddings=embeddings)\n",
        "ragas_score = calculate_ragas_score(basic_qa_result['answer_relevancy'],\n",
        "                                    basic_qa_result['faithfulness'], basic_qa_result['context_precision'],\n",
        "                                    basic_qa_result['context_recall'], basic_qa_result['answer_correctness'],\n",
        "                                    basic_qa_result['answer_similarity'])\n",
        "\n",
        "run_time = datetime.datetime.now()\n",
        "run_id = f\"{run_time.strftime('%Y%m%d%H%M%S')}_{uuid.uuid4()}\"\n",
        "\n",
        "result_dict = {\n",
        "    'context_precision': round(basic_qa_result['context_precision'],4),\n",
        "    'faithfulness': round(basic_qa_result['faithfulness'],4),\n",
        "    'answer_relevancy': round(basic_qa_result['answer_relevancy'],4),\n",
        "    'context_recall':round( basic_qa_result['context_recall'],4),\n",
        "    'answer_correctness':round( basic_qa_result['answer_correctness'],4),\n",
        "    'answer_similarity': round(basic_qa_result['answer_similarity'],4)\n",
        "}\n",
        "# Convert the dictionary to a DataFrame\n",
        "evaluate_df = pd.DataFrame([result_dict])\n",
        "evaluate_df['top_k']=retriever.top_k\n",
        "evaluate_df['model_temperature']= custom_model.temperature\n",
        "evaluate_df['max_token']= custom_model.max_tokens\n",
        "evaluate_df['top_p']= custom_model.top_p\n",
        "evaluate_df['embedding_model']= embeddings.model\n",
        "evaluate_df['overall_ragas_score']= round(ragas_score,4)\n",
        "evaluate_df['run_time']= run_time\n",
        "evaluate_df['run_id'] = run_id\n",
        "evaluate_df = evaluate_df.fillna(-111)\n",
        "\n",
        "all_questions_df = basic_qa_result.to_pandas()\n",
        "all_questions_df['run_id']= run_id\n",
        "\n",
        "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {environment_widget.value}_enriched_chatbot\")\n",
        "\n",
        "evaluation_metrics_sdf = spark.createDataFrame(all_questions_df)\n",
        "sanitized_run_id = run_time.strftime('%Y%m%d%H%M%S')\n",
        "\n",
        "evaluation_metrics_sdf.write.format(\"delta\").option(\"mergeSchema\", \"true\").saveAsTable(f\"{environment_widget.value}_local.{environment_widget.value}_enriched_chatbot.chatbot_evaluation_detailed_{sanitized_run_id}\")"
      ],
      "metadata": {
        "id": "yIpQWPnpdf5x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}